import os
import argparse
import asyncio

from dotenv import load_dotenv

from data_pipeline.models import DataManager, Retriever, Processor
from data_pipeline.crawling import Metaso,  RSS
from data_pipeline.processing import DummyProcessor

# read variables from .env
load_dotenv()
metaso_api_key = os.environ.get("METASO_API_KEY")

# BBC RSSæºåˆ—è¡¨
BBC_RSS_FEEDS = [
    "https://feeds.bbci.co.uk/news/world/rss.xml",
    "https://feeds.bbci.co.uk/news/uk/rss.xml",
    "https://feeds.bbci.co.uk/news/politics/rss.xml",
    "https://feeds.bbci.co.uk/news/health/rss.xml",
    "https://feeds.bbci.co.uk/news/education/rss.xml",
    "https://feeds.bbci.co.uk/news/technology/rss.xml",
    "https://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml",
    "https://feeds.bbci.co.uk/news/business/rss.xml",
    "https://feeds.bbci.co.uk/news/science_and_environment/rss.xml"
]

if __name__ == "__main__":
    # parse args
    parser = argparse.ArgumentParser(description="æ•°æ®çˆ¬å–å’Œå¤„ç†ç®¡é“")
    parser.add_argument("query", type=str, help="æœç´¢ä¸»é¢˜æˆ–äº‹ä»¶")
    parser.add_argument("--metaso_api_key", type=str, help="Metaso APIå¯†é’¥")
    parser.add_argument("--use_rss", action="store_true", help="å¯ç”¨RSSçˆ¬è™«")
    parser.add_argument("--rss_keywords", type=str, nargs="+", help="RSSå…³é”®è¯è¿‡æ»¤", default=["China", "Chinese"])
    parser.add_argument("--rss_feeds", type=str, nargs="+", help="è‡ªå®šä¹‰RSSæº", default=BBC_RSS_FEEDS)
    parser.add_argument("--max_concurrent", type=int, default=5, help="æœ€å¤§å¹¶å‘æ•°")

    args = parser.parse_args()
    query = args.query

    # è¦†ç›–.envé…ç½®
    if args.metaso_api_key:
        metaso_api_key = args.metaso_api_key

    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    data_manager = DataManager()
    retrievers: list[Retriever] = []
    processors: list[Processor] = [DummyProcessor(data_manager)]

    # æ·»åŠ DDGSçˆ¬è™«
    retrievers.append(DDGS(
        query,
        data_manager,
        max_concurrent=args.max_concurrent
    ))

    # æ·»åŠ Metasoçˆ¬è™«ï¼ˆå¦‚æœæä¾›äº†APIå¯†é’¥ï¼‰
    if metaso_api_key:
        retrievers.append(Metaso(
            metaso_api_key,
            query,
            data_manager,
            max_concurrent=args.max_concurrent
        ))

    # æ·»åŠ RSSçˆ¬è™«ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_rss:
        retrievers.append(RSS(
            query=query,
            data_manager=data_manager,
            rss_urls=args.rss_feeds,
            keywords=args.rss_keywords,
            max_concurrent=args.max_concurrent
        ))
        print(f"âœ… RSSçˆ¬è™«å·²å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯: {args.rss_keywords}")


    async def retrieve_all():
        """å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬è™«"""
        await asyncio.gather(*[retriever.retrieve() for retriever in retrievers])
        data_manager.finish_crawling()
        print("âœ… æ‰€æœ‰çˆ¬è™«ä»»åŠ¡å®Œæˆ")


    async def main():
        """ä¸»å¼‚æ­¥å‡½æ•°"""
        await asyncio.gather(
            retrieve_all(),
            *[processor.run() for processor in processors]
        )


    # è¿è¡Œä¸»ç¨‹åº
    print(f"ğŸš€ å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
    print(f"ğŸ“Š ä½¿ç”¨çˆ¬è™«: {[type(r).__name__ for r in retrievers]}")

    asyncio.run(main())

    # ä¿å­˜ç»“æœ
    output_file = f"data/{query.replace(' ', '_')}.json"
    data_manager.to_file(output_file)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - ç½‘é¡µæ•°é‡: {len(data_manager.webpages)}")
    print(f"   - å®ä½“æ•°é‡: {len(data_manager.entities)}")
    print(f"   - å­ä¸»é¢˜æ•°é‡: {len(data_manager.sub_themes)}")