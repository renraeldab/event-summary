#!/usr/bin/env python3
"""
æµ‹è¯•RSSçˆ¬è™«çš„ç‹¬ç«‹è„šæœ¬
æ”¾ç½®ä½ç½®ï¼šä¸crawlingæ–‡ä»¶å¤¹åŒçº§
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from data_pipeline.models import DataManager
from data_pipeline.crawling.rss import RSS

# BBC RSSæºåˆ—è¡¨
BBC_RSS_FEEDS = [
    "https://feeds.bbci.co.uk/news/world/rss.xml",
    "https://feeds.bbci.co.uk/news/uk/rss.xml",
    "https://feeds.bbci.co.uk/news/politics/rss.xml",
    "https://feeds.bbci.co.uk/news/health/rss.xml",
    "https://feeds.bbci.co.uk/news/education/rss.xml",
    "https://feeds.bbci.co.uk/news/technology/rss.xml",
    "https://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml",
    "https://feeds.bbci.co.uk/news/business/rss.xml",
    "https://feeds.bbci.co.uk/news/science_and_environment/rss.xml"
]


async def test_rss_basic():
    """æµ‹è¯•RSSçˆ¬è™«åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•RSSçˆ¬è™«åŸºæœ¬åŠŸèƒ½...")

    data_manager = DataManager()

    # åˆ›å»ºRSSçˆ¬è™«å®ä¾‹
    rss_crawler = RSS(
        query="ç§‘æŠ€æ–°é—»",
        data_manager=data_manager,
        rss_urls=BBC_RSS_FEEDS[:2],  # åªä½¿ç”¨å‰2ä¸ªæºè¿›è¡Œæµ‹è¯•
        keywords=["AI", "artificial intelligence"],
        max_concurrent=2,
        timeout=30
    )

    # è¿è¡Œçˆ¬è™«
    await rss_crawler.retrieve()

    # è¾“å‡ºç»“æœç»Ÿè®¡
    print(f"ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"   - è·å–çš„ç½‘é¡µæ•°é‡: {len(data_manager.webpages)}")
    print(f"   - çˆ¬è™«å‘ç°çš„æ¡ç›®æ•°: {len(rss_crawler.entries)}")

    # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
    print("\nğŸ“° å‰3ä¸ªç»“æœé¢„è§ˆ:")
    for i, (url, webpage) in enumerate(list(data_manager.webpages.items())[:3]):
        print(f"  title{i + 1}. {webpage['title']}")
        print(f"     æ¥æº: {url}")
        print(f"     æ—¶é—´æˆ³: {webpage['timestamp']}")
        print(f"     å†…å®¹é•¿åº¦: {len(webpage['content']) if webpage['content'] else 0} å­—ç¬¦")
        print(f"     æ‘˜è¦: {webpage['summary'][:100]}..." if webpage['summary'] else "æ— æ‘˜è¦")
        print()


async def test_rss_with_content_extraction():
    """æµ‹è¯•RSSçˆ¬è™«çš„å†…å®¹æå–åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•RSSçˆ¬è™«å†…å®¹æå–åŠŸèƒ½...")

    data_manager = DataManager()

    # ä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢å’Œå…³é”®è¯
    rss_crawler = RSS(
        query="BBCç§‘æŠ€æ–°é—»",
        data_manager=data_manager,
        rss_urls=["https://feeds.bbci.co.uk/news/technology/rss.xml"],  # åªä½¿ç”¨ç§‘æŠ€é¢‘é“
        keywords=["AI", "artificial intelligence"],
        max_concurrent=1,  # é™ä½å¹¶å‘æ•°é¿å…è¢«å°
        timeout=60
    )

    await rss_crawler.retrieve()

    # åˆ†æå†…å®¹æå–è´¨é‡
    total_pages = len(data_manager.webpages)
    pages_with_content = sum(1 for webpage in data_manager.webpages.values()
                             if webpage.get('content') and len(webpage['content']) > 100)

    print(f"ğŸ“Š å†…å®¹æå–è´¨é‡åˆ†æ:")
    print(f"   - æ€»ç½‘é¡µæ•°: {total_pages}")
    print(f"   - æˆåŠŸæå–å†…å®¹çš„ç½‘é¡µæ•°: {pages_with_content}")
    print(f"   - å†…å®¹æå–æˆåŠŸç‡: {pages_with_content / total_pages * 100:.1f}%")

    # æ˜¾ç¤ºå†…å®¹æå–ç¤ºä¾‹
    print("\nğŸ” å†…å®¹æå–ç¤ºä¾‹:")
    for i, (url, webpage) in enumerate(list(data_manager.webpages.items())[:2]):
        if webpage.get('content'):
            print(f"  {i + 1}. {webpage['title']}")
            print(f"     å†…å®¹é¢„è§ˆ: {webpage['content'][:200]}...")
            print()


async def test_rss_error_handling():
    """æµ‹è¯•RSSçˆ¬è™«çš„é”™è¯¯å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•RSSçˆ¬è™«é”™è¯¯å¤„ç†...")

    data_manager = DataManager()

    # ä½¿ç”¨æ— æ•ˆçš„URLæµ‹è¯•é”™è¯¯å¤„ç†
    invalid_feeds = [
        "https://invalid-rss-url-that-does-not-exist.com/feed.xml",
        "https://feeds.bbci.co.uk/news/technology/rss.xml"  # ä¿ç•™ä¸€ä¸ªæœ‰æ•ˆçš„ç”¨äºå¯¹æ¯”
    ]

    rss_crawler = RSS(
        query="é”™è¯¯å¤„ç†æµ‹è¯•",
        data_manager=data_manager,
        rss_urls=invalid_feeds,
        keywords=["test"],
        max_concurrent=2,
        timeout=5  # çŸ­è¶…æ—¶ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
    )

    await rss_crawler.retrieve()

    print(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆ")
    print(f"   - ä»æ— æ•ˆæºä¸­æ¢å¤ï¼ŒæˆåŠŸè·å–ç½‘é¡µæ•°: {len(data_manager.webpages)}")


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹RSSçˆ¬è™«æµ‹è¯•å¥—ä»¶")
    print("=" * 50)

    try:
        # æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½
        await test_rss_basic()

        # æµ‹è¯•2: å†…å®¹æå–
        await test_rss_with_content_extraction()

        # æµ‹è¯•3: é”™è¯¯å¤„ç†
        await test_rss_error_handling()

        print("=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¾“å‡ºå™ªéŸ³
    import logging

    logging.getLogger().setLevel(logging.WARNING)

    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())