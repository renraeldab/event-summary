{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606cf0bb",
   "metadata": {},
   "source": [
    "# A More Efficient JSON Data Editing with LLM Experiment\n",
    "## The current issue\n",
    "\n",
    "The `BaselineExtractor` and `BaselineGenerator` in `base.py` classes are responsible for organizing the incoming webpages into a more structured form of data using LLMs. However, the current implementation asks the LLM to do a complete update on the current JSON data even if only a minor change is needed and the rest of the data is exactly the same. The is very inefficient and consumes a lot of time and tokens.\n",
    "\n",
    "There was a study that proposed a patch framework for a more efficient way to edit JSON data with LLMs and called it [JSON Whisperer](https://arxiv.org/abs/2510.04717). The paper showed that rather than telling the LLM to generate the entire JSON document, it should instead generate [RFC 6902 diff patches](https://datatracker.ietf.org/doc/html/rfc6902) to indicate only the necessary changes needed for the JSON data and then updating the data locally using said changes.\n",
    "\n",
    "This report will experiment this idea and check its potential with a simplified version of our usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "912df01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import jsonpatch\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20ca43",
   "metadata": {},
   "source": [
    "## Experiment 1: Using Code Directly\n",
    "\n",
    "First we'll try to test this idea without LLM and compare the performance when done directly with python.\n",
    "\n",
    "We define an `initial_state` that will act as the JSON to be edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "328ef662",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"articles\": [\n",
    "        {\"id\": 1, \"title\": \"Old Title\"},\n",
    "        {\"id\": 2, \"title\": \"Another\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a496303",
   "metadata": {},
   "source": [
    "The code below is a very simple replication of what the `BaselineExtractor` and `BaselineGenerator` are currently doing, which is just rewriting the entire JSON data with when only a small change is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d7cb5084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline full regeneration: {\n",
      "  \"articles\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"New Title\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Another\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Baseline (full JSON regeneration)\n",
    "# New webpage says article 1 has a new title\n",
    "new_state = initial_state\n",
    "new_state = {\n",
    "    \"articles\": [\n",
    "        {\"id\": 1, \"title\": \"New Title\"},  # regenerated\n",
    "        {\"id\": 2, \"title\": \"Another\"}     # unchanged but re-emitted\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Baseline full regeneration:\", json.dumps(new_state, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaee6dd",
   "metadata": {},
   "source": [
    "The code below is a very simple replication of the proposed patch framework with RFC 6902 diff patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9b99e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch-based update: {\n",
      "  \"articles\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"New Title\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Another\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# JSON Whisperer Methodology (patch-based)\n",
    "# Patch generated by LLM (instead of full JSON)\n",
    "patch_ops = [\n",
    "    {\"op\": \"replace\", \"path\": \"/articles/0/title\", \"value\": \"New Title\"}\n",
    "]\n",
    "\n",
    "# Apply patch\n",
    "patch = jsonpatch.JsonPatch(patch_ops)\n",
    "new_state = patch.apply(initial_state)\n",
    "\n",
    "print(\"Patch-based update:\", json.dumps(new_state, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375474a9",
   "metadata": {},
   "source": [
    "Both have the exact same output but the approach is different. Now we test their performance.\n",
    "\n",
    "To automate the testing, we create an `update` object to indicate a change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4de9b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired update: change article 1's title\n",
    "update = {\"id\": 1, \"title\": \"New Title\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26dc7d",
   "metadata": {},
   "source": [
    "The previous simple implementations of the baseline and patch versions of the updating process has been turned into functions for automation of the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bed4e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: full JSON regeneration\n",
    "def baseline_update(state, update):\n",
    "    new_state = state\n",
    "    new_state = {\n",
    "        \"articles\": [\n",
    "            {\"id\": update[\"id\"], \"title\": update[\"title\"]},  # regenerated\n",
    "            {\"id\": 2, \"title\": \"Another\"}                   # unchanged but re-emitted\n",
    "        ]\n",
    "    }\n",
    "    return new_state\n",
    "\n",
    "# Patch-based: JSON Whisperer style\n",
    "def patch_update(state, update):\n",
    "    patch_ops = [\n",
    "        {\"op\": \"replace\", \"path\": \"/articles/0/title\", \"value\": update[\"title\"]}\n",
    "    ]\n",
    "    patch = jsonpatch.JsonPatch(patch_ops)\n",
    "    return patch.apply(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bddd8",
   "metadata": {},
   "source": [
    "A benchmark function is created, where it runs a set number of iterations that changes the JSON data using the different methods and outputs the performance of both. The performance is measured through runtime and the number of characters in the output. We also check if the output of both methods are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f2d1e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def benchmark(n_runs=10000):\n",
    "    # Baseline\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        baseline_result = baseline_update(initial_state, update)\n",
    "    baseline_time = time.perf_counter() - start\n",
    "    baseline_size = len(json.dumps(baseline_result))\n",
    "\n",
    "    # Patch-based\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        patch_result = patch_update(initial_state, update)\n",
    "    patch_time = time.perf_counter() - start\n",
    "    patch_size = len(json.dumps(patch_result))\n",
    "\n",
    "    print(\"=== Performance Comparison ===\")\n",
    "    print(f\"Baseline runtime: {baseline_time:.6f} seconds for {n_runs} runs\")\n",
    "    print(f\"Patch runtime:    {patch_time:.6f} seconds for {n_runs} runs\")\n",
    "    print(f\"\\nBaseline JSON size: {baseline_size} chars\")\n",
    "    print(f\"Patch JSON size:    {patch_size} chars\")\n",
    "\n",
    "    # Verify correctness\n",
    "    same = (baseline_result == patch_result)\n",
    "    print(f\"\\nFinal states identical: {same}\")\n",
    "    if same:    \n",
    "        print(\"\\nFinal json state from both:\\n\", json.dumps(baseline_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f45a8",
   "metadata": {},
   "source": [
    "10000 iterations will be done to show noticeable time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a1a85a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performance Comparison ===\n",
      "Baseline runtime: 0.002868 seconds for 10000 runs\n",
      "Patch runtime:    0.157197 seconds for 10000 runs\n",
      "\n",
      "Baseline JSON size: 78 chars\n",
      "Patch JSON size:    78 chars\n",
      "\n",
      "Final states identical: True\n",
      "\n",
      "Final json state from both:\n",
      " {\n",
      "  \"articles\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"New Title\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Another\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark\n",
    "benchmark(n_runs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ecd8c",
   "metadata": {},
   "source": [
    "We see that both methods have the exact same output. However, the proposed patch method is much slower than the baseline method. This is because we are directly using code to implement the methods and python is very efficient in performing JSON dumps compared to editing JSON through patches.\n",
    "\n",
    "The proposed patch method is supposed to be used with LLMs, so now we move on to the next part of the experiment using LLMs.\n",
    "\n",
    "## Experiment 2: Using LLM\n",
    "\n",
    "This eperiment will use the OpenAI package for the LLM\n",
    "\n",
    "First, we configure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3ae28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_base_url = os.environ.get(\"OPENAI_BASE_URL\")\n",
    "openai_model = os.environ.get(\"OPENAI_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a1fdb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46023433",
   "metadata": {},
   "source": [
    "Next, we build the prompts that will be passed into OpenAI.\n",
    "\n",
    "For the baseline method, we ask OpenAI to update a JSON data and regernate the full JSON.\n",
    "\n",
    "For the patch method, we instead tell OpenAI to only give us the RFC6902 patch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eaeaef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prompts\n",
    "def baseline_prompt(state, update):\n",
    "    return f\"\"\"You are an extractor. Current JSON:\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "Update: Article {update['id']} has new title \"{update['title']}\".\n",
    "Regenerate the full JSON with this change.\n",
    "\"\"\"\n",
    "\n",
    "# Patch prompts\n",
    "def patch_prompt(state, update):\n",
    "    return f\"\"\"You are an extractor. Current JSON:\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "Update: Article {update['id']} has new title \"{update['title']}\".\n",
    "Instead of regenerating, output RFC6902 patch operations only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ad041",
   "metadata": {},
   "source": [
    "Sometimes LLMs tend to have a wrapper in the output, so we'll make a function to clean the output for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "96778dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust patch output cleaner\n",
    "def clean_patch_output(output_text):\n",
    "    # Strip markdown fences if present\n",
    "    text = output_text.strip().strip(\"`\")\n",
    "    if text.startswith(\"json\"):\n",
    "        text = text.lstrip(\"json\").strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Invalid JSON from model: {e}\\n{text}\")\n",
    "\n",
    "    # Ensure it's a list of dicts with 'op'\n",
    "    if not isinstance(parsed, list):\n",
    "        raise RuntimeError(f\"Patch output is not a list: {parsed}\")\n",
    "    if not all(isinstance(item, dict) and \"op\" in item for item in parsed):\n",
    "        raise RuntimeError(f\"Patch output is not valid RFC6902 ops: {parsed}\")\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815e2a7",
   "metadata": {},
   "source": [
    "Other than measruing the runtime and the output size, we will also measure the tokens. We create a class to track the tokens used by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9763d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking tokens\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def query_llm(self, prompt):\n",
    "        start = time.perf_counter()\n",
    "        response = client.chat.completions.create(\n",
    "            model=openai_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        latency = time.perf_counter() - start\n",
    "        output_text = response.choices[0].message.content\n",
    "\n",
    "        # Track tokens if usage info is available\n",
    "        tokens_used = None\n",
    "        if response.usage:\n",
    "            tokens_used = response.usage.total_tokens\n",
    "            self.total_tokens += tokens_used\n",
    "\n",
    "        return latency, output_text, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b85593",
   "metadata": {},
   "source": [
    "We define a function to benchmark the performance of the baseline method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "97c84084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline benchmark\n",
    "def benchmarkLLMBaseline(tracker):\n",
    "    latency, output_text, tokens = tracker.query_llm(\n",
    "        baseline_prompt(initial_state, update)\n",
    "    )\n",
    "    size = len(output_text)\n",
    "    try:\n",
    "        parsed_json = json.loads(output_text)\n",
    "    except json.JSONDecodeError:\n",
    "        parsed_json = None\n",
    "\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"size\": size,\n",
    "        \"tokens\": tokens,\n",
    "        \"output_text\": output_text,\n",
    "        \"parsed_json\": parsed_json\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb184a",
   "metadata": {},
   "source": [
    "For the patch method, there are two steps: The LLM first generates the RFC 6902 diff patches and then we use the patches to apply the changes to the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1d4b297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch benchmark\n",
    "def benchmarkLLMPatch(tracker):\n",
    "    latency, output_text, tokens = tracker.query_llm(\n",
    "        patch_prompt(initial_state, update)\n",
    "    )\n",
    "    size = len(output_text)\n",
    "\n",
    "    # Clean and apply patch ops\n",
    "    apply_start = time.perf_counter()\n",
    "    patched_state = None\n",
    "    try:\n",
    "        ops = clean_patch_output(output_text)\n",
    "        patch = jsonpatch.JsonPatch(ops)\n",
    "        patched_state = patch.apply(initial_state)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to apply patch: {e}\")\n",
    "    apply_latency = time.perf_counter() - apply_start\n",
    "    total_latency = latency + apply_latency\n",
    "\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"apply_latency\": apply_latency,\n",
    "        \"total_latency\": total_latency,\n",
    "        \"size\": size,\n",
    "        \"tokens\": tokens,\n",
    "        \"output_text\": output_text,\n",
    "        \"patched_state\": patched_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1fc83d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Tracker\n",
    "tracker = TokenTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27912774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the different methods\n",
    "baseline_result = benchmarkLLMBaseline(tracker)\n",
    "patch_result = benchmarkLLMPatch(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56371",
   "metadata": {},
   "source": [
    "We now take a look at the outputs from the LLM for the different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8e9ad188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline output:\n",
      " {\n",
      "  \"articles\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"New Title\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Another\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# LLM output for baseline method\n",
    "print(\"Baseline output:\\n\", baseline_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4451de4",
   "metadata": {},
   "source": [
    "For the baseline method, we already get the final output with the required changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8ee15cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch output:\n",
      " ```json\n",
      "[\n",
      "  {\n",
      "    \"op\": \"replace\",\n",
      "    \"path\": \"/articles/0/title\",\n",
      "    \"value\": \"New Title\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(\"Patch output:\\n\", patch_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca1d1a",
   "metadata": {},
   "source": [
    "For the patch method, we are only getting the RFC 6902 diff patches which will be used to further update the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "56d1e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Performance Comparison ===\n",
      "Baseline latency:          2.981 seconds\n",
      "\n",
      "Patch latency (LLM only):  1.976 seconds\n",
      "Patch apply latency:       0.000059 seconds\n",
      "Patch total latency:       1.976 seconds\n",
      "\n",
      "Baseline output size: 130 chars\n",
      "Patch output size:    102 chars\n",
      "\n",
      "Baseline tokens used: 152\n",
      "Patch tokens used:    144\n"
     ]
    }
   ],
   "source": [
    "print(\"=== OpenAI Performance Comparison ===\")\n",
    "print(f\"Baseline latency:          {baseline_result['latency']:.3f} seconds\")\n",
    "print(f\"\\nPatch latency (LLM only):  {patch_result['latency']:.3f} seconds\")\n",
    "print(f\"Patch apply latency:       {patch_result['apply_latency']:.6f} seconds\")\n",
    "print(f\"Patch total latency:       {patch_result['total_latency']:.3f} seconds\")\n",
    "print(f\"\\nBaseline output size: {baseline_result['size']} chars\")\n",
    "print(f\"Patch output size:    {patch_result['size']} chars\")\n",
    "print(f\"\\nBaseline tokens used: {baseline_result['tokens']}\")\n",
    "print(f\"Patch tokens used:    {patch_result['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394570a",
   "metadata": {},
   "source": [
    "Based on the results, we can see that even though the patch method is a two step process while the baseline is just one step, the patch method was completed faster than the baseline method. Furthermore, patch method also has a smaller size and used less tokens than the baseline method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "71c10b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final states identical: True\n",
      "\n",
      "Final formatted json state from both:\n",
      " {\n",
      "  \"articles\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"New Title\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Another\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "same = (baseline_result[\"parsed_json\"] == patch_result[\"patched_state\"])\n",
    "print(f\"Final states identical: {same}\")\n",
    "if same:    \n",
    "    print(\"\\nFinal formatted json state from both:\\n\", baseline_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95104cd8",
   "metadata": {},
   "source": [
    "The output is also exactly the same.  This shows that the proposed patch method is indeed more efficient than the baseline method which still having the exact same output as seen below.\n",
    "\n",
    "But the prompts used in the project are much more complicated that the experiments done so far. So let's run the same experiment with prompts as complex as the ones used in the project.\n",
    "\n",
    "## Experiment 3: Complex Prompts\n",
    "\n",
    "The prompts used in the project are much richer and detailed. They defined schemas, provided documents, and so much more while giving specific intructions on the desired output. One of the prompts was related to updating the entity recognition feature, so we'll use a similar version of that with the same richness for the prompt in this experiment.\n",
    "\n",
    "First, we change `initial_state` and `update` to work with the entity recognition JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4bc36938",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"entities\": [\n",
    "        {\"type\": \"Person\", \"name\": \"Alice\", \"description\": \"Researcher\"},\n",
    "        {\"type\": \"Organization\", \"name\": \"OpenAI\", \"description\": \"AI lab\"}\n",
    "    ]\n",
    "}\n",
    "update = {\"id\": 1, \"title\": \"New Discovery\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9c84c",
   "metadata": {},
   "source": [
    "We change the prompt for the baseline method and use the rich prompt similar to the ones used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1f806194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_prompt(state, update):\n",
    "    return f\"\"\"# Task\n",
    "Extract entities from the given document and update the existing list.\n",
    "- Valid entity types and their schemas are defined below.\n",
    "- Maintain relevance, avoid duplication, keep the list concise.\n",
    "- Always include the new information provided in the query.\n",
    "\n",
    "# Entity Definition\n",
    "```python\n",
    "class Person(TypedDict):\n",
    "    type: Literal[\"Person\"]\n",
    "    name: str\n",
    "    description: str\n",
    "class Organization(TypedDict):\n",
    "    type: Literal[\"Organization\"]\n",
    "    name: str\n",
    "    description: str\n",
    "class Event(TypedDict):\n",
    "    type: Literal[\"Event\"]\n",
    "    name: str\n",
    "    description: str\n",
    "    time: str\n",
    "\n",
    "Query\n",
    "\n",
    "Update entity list with new information: \"{update['title']}\"\n",
    "Document\n",
    "\n",
    "<DOCUMENT START> {json.dumps(state, indent=2)} <DOCUMENT END>\n",
    "Current Entities\n",
    "{json.dumps(state, indent=2)}\n",
    "Return the full updated entity list in JSON. DO NOT output anything else. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6b883",
   "metadata": {},
   "source": [
    "For the patch prompt, we use a similar prompt to the baseline prompt and instead of regenerating the entire JSON, just output the RFC6902 patch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fe55278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_prompt(state, update):\n",
    "    return f\"\"\"# Task\n",
    "Generate RFC6902 JSON Patch operations to update the current entity list.\n",
    "- Valid entity types and their schemas are defined below.\n",
    "- Always include the new information provided in the query.\n",
    "- If adding an Event, set description=\"\" and time=\"\" unless explicit values are given.\n",
    "- Do not regenerate the full list, only output patch operations.\n",
    "\n",
    "# Entity Definition\n",
    "```python\n",
    "class Person(TypedDict):\n",
    "    type: Literal[\"Person\"]\n",
    "    name: str\n",
    "    description: str\n",
    "class Organization(TypedDict):\n",
    "    type: Literal[\"Organization\"]\n",
    "    name: str\n",
    "    description: str\n",
    "class Event(TypedDict):\n",
    "    type: Literal[\"Event\"]\n",
    "    name: str\n",
    "    description: str\n",
    "    time: str\n",
    "\n",
    "Query\n",
    "\n",
    "Add new information: \"{update['title']}\"\n",
    "Current Entities\n",
    "\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "Output\n",
    "\n",
    "Return only a JSON array of RFC6902 operations (e.g. add/replace/remove). Do not include explanations or the full entity list. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e86c1e",
   "metadata": {},
   "source": [
    "Same as before, we clean the output from the LLM to make it useable later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1e19dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_baseline_output(output_text: str):\n",
    "    \"\"\"Strip markdown fences and load baseline JSON output.\"\"\"\n",
    "    text = output_text.strip().strip(\"`\")\n",
    "    if text.startswith(\"json\"):\n",
    "        text = text.lstrip(\"json\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Invalid baseline JSON: {e}\\n{text}\")\n",
    "\n",
    "\n",
    "def clean_patch_output(output_text: str):\n",
    "    \"\"\"Strip markdown fences and validate RFC6902 patch operations.\"\"\"\n",
    "    text = output_text.strip().strip(\"`\")\n",
    "    if text.startswith(\"json\"):\n",
    "        text = text.lstrip(\"json\").strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Invalid JSON from model: {e}\\n{text}\")\n",
    "\n",
    "    # Ensure it's a list of dicts with 'op'\n",
    "    if not isinstance(parsed, list):\n",
    "        raise RuntimeError(f\"Patch output is not a list: {parsed}\")\n",
    "    if not all(isinstance(item, dict) and \"op\" in item for item in parsed):\n",
    "        raise RuntimeError(f\"Patch output is not valid RFC6902 ops: {parsed}\")\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706944ca",
   "metadata": {},
   "source": [
    "We define the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d321326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkBaseline(tracker):\n",
    "    latency, output_text, tokens = tracker.query_llm(baseline_prompt(initial_state, update))\n",
    "    size = len(output_text)\n",
    "    parsed_json = clean_baseline_output(output_text)\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"size\": size,\n",
    "        \"tokens\": tokens,\n",
    "        \"output_text\": output_text,\n",
    "        \"parsed_json\": parsed_json\n",
    "    }\n",
    "\n",
    "def benchmarkPatch(tracker):\n",
    "    latency, output_text, tokens = tracker.query_llm(patch_prompt(initial_state, update))\n",
    "    size = len(output_text)\n",
    "    apply_start = time.perf_counter()\n",
    "    patched_state = None\n",
    "    try:\n",
    "        ops = clean_patch_output(output_text)\n",
    "        patch = jsonpatch.JsonPatch(ops)\n",
    "        patched_state = patch.apply(initial_state)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to apply patch: {e}\")\n",
    "    apply_latency = time.perf_counter() - apply_start\n",
    "    total_latency = latency + apply_latency\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"apply_latency\": apply_latency,\n",
    "        \"total_latency\": total_latency,\n",
    "        \"size\": size,\n",
    "        \"tokens\": tokens,\n",
    "        \"output_text\": output_text,\n",
    "        \"patched_state\": patched_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db65e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = benchmarkBaseline(tracker)\n",
    "patch_result = benchmarkPatch(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bc7d6",
   "metadata": {},
   "source": [
    "We see that the output from the LLM for the baseline method is pretty much the final output with the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cb33d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline output:\n",
      " {\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"type\": \"Person\",\n",
      "      \"name\": \"Alice\",\n",
      "      \"description\": \"Researcher\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Organization\",\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"description\": \"AI lab\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Event\",\n",
      "      \"name\": \"New Discovery\",\n",
      "      \"description\": \"\",\n",
      "      \"time\": \"\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline output:\\n\", baseline_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9b111",
   "metadata": {},
   "source": [
    "The output from the LLM of the patch method is the RFC 6902 diff patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4087d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch output:\n",
      " [\n",
      "  {\n",
      "    \"op\": \"add\",\n",
      "    \"path\": \"/entities/2\",\n",
      "    \"value\": {\n",
      "      \"type\": \"Event\",\n",
      "      \"name\": \"New Discovery\",\n",
      "      \"description\": \"\",\n",
      "      \"time\": \"\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(\"Patch output:\\n\", patch_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b80c7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Performance Comparison ===\n",
      "Baseline latency: 3.527 seconds\n",
      "\n",
      "Patch latency (LLM only):  2.282 seconds\n",
      "Patch apply latency:       0.000059 seconds\n",
      "Patch total latency:       2.282 seconds\n",
      "\n",
      "Baseline output size: 322 chars\n",
      "Patch output size:    172 chars\n",
      "\n",
      "Baseline tokens used: 427\n",
      "Patch tokens used:    349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== OpenAI Performance Comparison ===\")\n",
    "print(f\"Baseline latency: {baseline_result['latency']:.3f} seconds\")\n",
    "print(f\"\\nPatch latency (LLM only):  {patch_result['latency']:.3f} seconds\")\n",
    "print(f\"Patch apply latency:       {patch_result['apply_latency']:.6f} seconds\")\n",
    "print(f\"Patch total latency:       {patch_result['total_latency']:.3f} seconds\")\n",
    "print(f\"\\nBaseline output size: {baseline_result['size']} chars\")\n",
    "print(f\"Patch output size:    {patch_result['size']} chars\")\n",
    "print(f\"\\nBaseline tokens used: {baseline_result['tokens']}\")\n",
    "print(f\"Patch tokens used:    {patch_result['tokens']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935eb12",
   "metadata": {},
   "source": [
    "We see that the patch method is still more efficient even with more rich and complex prompts. This shows that the proposal of the JSON Whisperer does indeed make JSON editing with LLMs much more efficient and is something to consider for future improvements and development of this project.\n",
    "\n",
    "### Something to note:\n",
    "The output from LLMs can be very different, even when using the exact same prompt twice with a lot of details, to a point that the behavior can sometimes be very unpredictable. This, along with network traffic and other uncontrollable factors, could potentially change the performance of either methods. However, the difference in performance seems to be large enough that we can expect the patch method to be more efficient than the baseline method in most case, if not all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
